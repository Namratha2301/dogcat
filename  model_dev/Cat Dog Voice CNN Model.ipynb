{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1efce4",
   "metadata": {},
   "source": [
    "# Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde87e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# from memory_profiler import memory_usage\n",
    "import IPython.display as ipd\n",
    "# % pylab inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob \n",
    "import librosa\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "from keras.layers import Convolution2D, MaxPooling2D,  Conv2D, LeakyReLU\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from glob import glob\n",
    "import librosa\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "import gc\n",
    "import librosa.display\n",
    "# from path import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb032fa7",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa009286",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = os.listdir(\"path-to-cat-dataset-train\")\n",
    "cat_files = pd.DataFrame(filelist)\n",
    "metadata = pd.read_csv(\"path-to-metadata-file\")\n",
    "filelist = os.listdir(\"path-to-dog-dataset-train\")\n",
    "dog_files = pd.DataFrame(filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b231b363",
   "metadata": {},
   "source": [
    "# Merging the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8fec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_files['label']='0'\n",
    "dog_files['label']='1'\n",
    "df = pd.concat([dog_files,cat_files],ignore_index=True)\n",
    "df = df.rename(columns={0:'file'})\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df09f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Class Balance\n",
    "\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Erroneous Folders\n",
    "\n",
    "df[df['file']=='.DS_Store']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe9cf9",
   "metadata": {},
   "source": [
    "# Generating Train and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59cb0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[:225]\n",
    "val = df[225:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb669de5",
   "metadata": {},
   "source": [
    "# Generating MelSpectogram for Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf543d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='drive/MyDrive/ProjectMGN/CatandDog/cats_dogs/train/dog/dog_barking_0.wav'\n",
    "X, sample_rate = librosa.load(filename, sr=None, res_type='kaiser_fast')\n",
    "S = librosa.feature.melspectrogram(y=X, sr=sample_rate)\n",
    "librosa.display.specshow(librosa.power_to_db(S, ref=np.max), x_axis='time', y_axis='mel', fmin=50, fmax=280)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552c74af",
   "metadata": {},
   "source": [
    "# Generating MelSpectogram for Entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e650f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although this function was modified and many parameteres were explored with, most of it\n",
    "# came from Source 18 (sources in the READ.ME)\n",
    "\n",
    "def images(files):\n",
    "    \n",
    "    # We define the audiofile from the rows of the dataframe when we iterate through\n",
    "    # every row of our dataframe for train, val and test\n",
    "    try:\n",
    "      audiofile = 'drive/MyDrive/ProjectMGN/CatandDog/cats_dogs/train/cat/'+str(files.file)\n",
    "      X, sample_rate = librosa.load(audiofile, sr=None, res_type='kaiser_fast')\n",
    "    except FileNotFoundError:\n",
    "      audiofile = 'drive/MyDrive/ProjectMGN/CatandDog/cats_dogs/train/dog/'+str(files.file)\n",
    "      X, sample_rate = librosa.load(audiofile, sr=None, res_type='kaiser_fast')\n",
    "    print(audiofile)\n",
    "   \n",
    "    # Setting the size of the image\n",
    "    fig = plt.figure(figsize=[1,1])\n",
    "    \n",
    "    # This is to get rid of the axes and only get the picture \n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    \n",
    "    # This is the melspectrogram from the decibels with a linear relationship\n",
    "    # Setting min and max frequency to account for human voice frequency\n",
    "    S = librosa.feature.melspectrogram(y=X, sr=sample_rate)\n",
    "    librosa.display.specshow(librosa.power_to_db(S, ref=np.max), x_axis='time', y_axis='mel', fmin=50, fmax=280)\n",
    "    \n",
    "    # Here we choose the path and the name to save the file, we will change the path when\n",
    "    # using the function for train, val and test to make the function easy to use and output\n",
    "    # the images in different folders to use later with a generator\n",
    "    name = files.file\n",
    "    file  = 'drive/MyDrive/ProjectMGN/CatDogTransformed/train/' + str(name) + '.jpg'\n",
    "    print(file)\n",
    "    # Here we finally save the image file choosing the resolution \n",
    "    plt.savefig(file, dpi=500, bbox_inches='tight',pad_inches=0)\n",
    "    \n",
    "    # Here we close the image because otherwise we get a warning saying that the image stays\n",
    "    # open and consumes memory\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.apply(images,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a7555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although this function was modified and many parameteres were explored with, most of it\n",
    "# came from Source 18 (sources in the READ.ME)\n",
    "\n",
    "def images(files):\n",
    "    \n",
    "    # We define the audiofile from the rows of the dataframe when we iterate through\n",
    "    # every row of our dataframe for train, val and test\n",
    "    try:\n",
    "      audiofile = 'drive/MyDrive/ProjectMGN/CatandDog/cats_dogs/train/cat/'+str(files.file)\n",
    "      X, sample_rate = librosa.load(audiofile, sr=None, res_type='kaiser_fast')\n",
    "    except FileNotFoundError:\n",
    "      audiofile = 'drive/MyDrive/ProjectMGN/CatandDog/cats_dogs/train/dog/'+str(files.file)\n",
    "      X, sample_rate = librosa.load(audiofile, sr=None, res_type='kaiser_fast')\n",
    "    print(audiofile)\n",
    "   \n",
    "    # Setting the size of the image\n",
    "    fig = plt.figure(figsize=[1,1])\n",
    "    \n",
    "    # This is to get rid of the axes and only get the picture \n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    \n",
    "    # This is the melspectrogram from the decibels with a linear relationship\n",
    "    # Setting min and max frequency to account for human voice frequency\n",
    "    S = librosa.feature.melspectrogram(y=X, sr=sample_rate)\n",
    "    librosa.display.specshow(librosa.power_to_db(S, ref=np.max), x_axis='time', y_axis='mel', fmin=50, fmax=280)\n",
    "    \n",
    "    # Here we choose the path and the name to save the file, we will change the path when\n",
    "    # using the function for train, val and test to make the function easy to use and output\n",
    "    # the images in different folders to use later with a generator\n",
    "    name = files.file\n",
    "    file  = 'drive/MyDrive/ProjectMGN/CatDogTransformed/test/' + str(name) + '.jpg'\n",
    "    print(file)\n",
    "    # Here we finally save the image file choosing the resolution \n",
    "    plt.savefig(file, dpi=500, bbox_inches='tight',pad_inches=0)\n",
    "    \n",
    "    # Here we close the image because otherwise we get a warning saying that the image stays\n",
    "    # open and consumes memory\n",
    "    plt.close()\n",
    "\n",
    "val.apply(images,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea9b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to change the file names to the image names to use them later \n",
    "def make_jpg(files):\n",
    "    return str(files.split('.')[0])+'.wav.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af47ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6bf688",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['file']=train['file'].apply(make_jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda81743",
   "metadata": {},
   "outputs": [],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874dd20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val['file']=val['file'].apply(make_jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255.)\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "    dataframe=train,\n",
    "    directory=\"drive/MyDrive/ProjectMGN/CatDogTransformed/train\",\n",
    "    x_col=\"file\",\n",
    "    y_col=\"label\",\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d2ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator=datagen.flow_from_dataframe(\n",
    "    dataframe=val,\n",
    "    directory=\"drive/MyDrive/ProjectMGN/CatDogTransformed/test\",\n",
    "    x_col=\"file\",\n",
    "    y_col=\"label\",\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c3361",
   "metadata": {},
   "source": [
    "# Developing CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aadf42a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building our model \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c55c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b616d7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 569,410\n",
      "Trainable params: 569,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96daacea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting our CNN with 250 epochs and setting the results to history for visuals\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=10,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=5,\n",
    "                    epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
